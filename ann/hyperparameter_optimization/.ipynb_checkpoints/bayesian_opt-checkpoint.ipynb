{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3GkjK8vzJwM"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from statistics import mean \n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6abZQdSVZ6-"
   },
   "outputs": [],
   "source": [
    "def data_processing(X1, X2, y, k=None, ratio=None):\n",
    "  INPUT_COUNT = X1.shape[1]\n",
    "  #next two line of codes take mean of one input-feature set and subtracts it from all of the elements and the returns the absolute values for normalized phase diff\n",
    "  X1 = X1 - np.transpose(X1.mean(axis=1).repeat(INPUT_COUNT).reshape(INPUT_COUNT, -1))\n",
    "  X1 = np.absolute(X1)\n",
    "  #next two line of codes take mean of one input-feature set and subtracts it from all of the elements and the returns the absolute values for normalized amplitudes\n",
    "  X2 = X2 - np.transpose(X2.mean(axis=1).repeat(INPUT_COUNT).reshape(INPUT_COUNT, -1))\n",
    "  X2 = np.absolute(X2)\n",
    "\n",
    "  X = np.append(X1, X2, axis=1)\n",
    "  del X1, X2\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)#creating test train split\n",
    "  del X, y\n",
    "\n",
    "  X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.2)#creating test validation split\n",
    "  \n",
    "  INPUT_COUNT = X_test.shape[1]\n",
    "  train_samples = np.append(X_train, y_train.reshape(-1, 1), axis=1)#appending input features and labels into one numpy array\n",
    "  del X_train\n",
    "\n",
    "\n",
    "\n",
    "  nonPS = train_samples[y_train==0]#saving the example sets with label nonPS into one array\n",
    "  PS = train_samples[y_train==1]#saving the example sets with label PS into other array\n",
    "  del y_train\n",
    "\n",
    "\n",
    "  nonPS_sample = resample(nonPS, n_samples=int(k*ratio*len(PS)))#nonPS example-sets in the current data-set chosen with replacement, (k*ratio*(number of PS pixels)) in size\n",
    "  PS_sample = resample(PS, n_samples=int(k*len(PS)))#PS example-sets in the current data-set chosen with replacement, (k * (number of PS pixels)) in size\n",
    "  del nonPS, PS\n",
    "  final_train_sample = np.append(nonPS_sample, PS_sample, axis=0)#appending PS and nonPS examples\n",
    "  del nonPS_sample, PS_sample\n",
    "\n",
    "  np.random.shuffle(final_train_sample)#shuffling the appended array\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "  X_val = torch.tensor(X_val)\n",
    "  X_test = torch.tensor(X_test)\n",
    "  y_val = torch.tensor(y_val)\n",
    "  y_test = torch.tensor(y_test)\n",
    "  final_train_sample = torch.tensor(final_train_sample)\n",
    "\n",
    "  \n",
    "\n",
    "  return INPUT_COUNT, final_train_sample, X_val, X_test, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGWndi3xVd-L"
   },
   "outputs": [],
   "source": [
    "def train(INPUT_COUNT, train_sample, X_val, y_val, h_layers, nodes_phl, p):\n",
    "    \n",
    "    \n",
    "\n",
    "    #instantizing network with 1 hidden layer, 1024 nodes per hidden layer and 0.0 dropout probability as obtained from hyper-parameter tuning\n",
    "    net = Net(INPUT_COUNT, h_layers, nodes_phl, p).cuda()\n",
    "\n",
    "    #intializing optimimzer instance\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "    best_val_loss=1.\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    #setting threshold epochs to terminate the training after which no improvement in the chosen validation loss is observed \n",
    "    THRESHOLD = 200\n",
    "\n",
    "    while(True):\n",
    "      \n",
    "      epoch_training_loss= one_epoch(INPUT_COUNT, net, optimizer, train_sample[:,0:-1], train_sample[:,-1], True)\n",
    "      \n",
    "      epoch_val_loss= one_epoch(INPUT_COUNT, net, optimizer, X_val, y_val, False)\n",
    "\n",
    "      counter, best_val_loss = update_counter(counter, epoch_val_loss, best_val_loss)\n",
    "\n",
    "      if(counter==THRESHOLD):\n",
    "        break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uc8E9dY1xAy9"
   },
   "outputs": [],
   "source": [
    "def update_counter(counter, epoch_val_loss, best_val_loss):\n",
    "\n",
    "  is_best_loss = epoch_val_loss < best_val_loss\n",
    "  best_val_loss = min(epoch_val_loss, best_val_loss)\n",
    "  print(epoch_val_loss)\n",
    "  print(best_val_loss)\n",
    "  if is_best_loss:\n",
    "    counter=0\n",
    "  \n",
    "  counter += 1\n",
    "\n",
    "  return counter, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8xtuwSgj4cB"
   },
   "outputs": [],
   "source": [
    "#class to instantize network object with \"h_layers\" hidden layers, \"nodes_phl\" nodes per hidden layer  and dropout layer with probability \"p\" \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, INPUT_COUNT, h_layers, nodes_phl, dropout_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h_layers = h_layers\n",
    "        \n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(INPUT_COUNT, nodes_phl),\n",
    "            nn.BatchNorm1d(nodes_phl),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden = nn.ModuleList()\n",
    "        for k in range(h_layers-1):\n",
    "            self.hidden.append(nn.Sequential(\n",
    "            nn.Linear(nodes_phl, nodes_phl),\n",
    "            nn.BatchNorm1d(nodes_phl),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "        \n",
    "        self.output = nn.Linear(nodes_phl, 1) if h_layers>0 else nn.Linear(INPUT_COUNT, 1)\n",
    "        \n",
    "        self.drop_layer = nn.Dropout(p=dropout_p)\n",
    "    \n",
    "    def forward(self, X):#forward propogation through network\n",
    "\n",
    "        if self.h_layers:\n",
    "          X = self.input(X)\n",
    "          X = self.drop_layer(X)\n",
    "        for layer in self.hidden:\n",
    "          X = layer(X)\n",
    "          X = self.drop_layer(X)\n",
    "        X = self.output(X)\n",
    "        return torch.sigmoid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1sSwBb3Ej-f_"
   },
   "outputs": [],
   "source": [
    "#class to create F1_loss object instance\n",
    "class Fbeta_Loss(nn.Module):\n",
    "    \n",
    "    def __init__(self, beta, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, y_pred, y_true,):\n",
    "        \n",
    "        tp = (y_true * y_pred).sum()\n",
    "        fp = ((1 - y_true) * y_pred).sum()\n",
    "        fn = (y_true * (1 - y_pred)).sum()\n",
    "\n",
    "\n",
    "        fbeta = (1 + self.beta*self.beta)*tp / ((1 + self.beta*self.beta)*tp + (self.beta*self.beta)*fp + fn + self.epsilon)\n",
    "        return 1 - fbeta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r91QjRVcj-gF"
   },
   "outputs": [],
   "source": [
    "#function to pass batch through network with some additional arguments\n",
    "def fwd_pass(net, optimizer, batch_X, batch_y, train):\n",
    "    \n",
    "    if train:\n",
    "      net.train()\n",
    "      net.zero_grad()\n",
    "      outputs = net(batch_X)\n",
    "      loss = loss_function(outputs, batch_y)\n",
    "      loss.backward()\n",
    "      optimizer.step()  \n",
    "    \n",
    "    else:\n",
    "      net.eval()\n",
    "      with torch.no_grad():\n",
    "        outputs = net(batch_X)\n",
    "      loss = loss_function(outputs, batch_y)    \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Tr4OmCdyPiR"
   },
   "outputs": [],
   "source": [
    "# completing one epoch through the given dataset\n",
    "def one_epoch(INPUT_COUNT, net, optimizer, X, y, train=False):\n",
    "  LOSS = []\n",
    "  BATCH_SIZE = 32768\n",
    "  for i in range(0, len(X), BATCH_SIZE):\n",
    "    batch_X = X[i:i+BATCH_SIZE].view(-1, INPUT_COUNT)\n",
    "    batch_y = y[i:i+BATCH_SIZE].view(-1)\n",
    "    \n",
    "    batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "\n",
    "    loss = fwd_pass(net, optimizer, batch_X, batch_y, train)\n",
    "    LOSS.append(loss)\n",
    "    \n",
    "  LOSS = torch.mean(torch.stack(LOSS))\n",
    "  \n",
    "  return LOSS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOxyDIpg_6xJ"
   },
   "outputs": [],
   "source": [
    "#mounting my drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BwTFM-el8CE"
   },
   "outputs": [],
   "source": [
    "X1 =  np.load('/content/drive/My Drive/PSInSAR/normalized_phase_diff.npy')#loading normalized phase diff interferograms reshaped as (-1, 37) aka (-1, INPUT_CHANNELS)\n",
    "X2 = np.load('/content/drive/My Drive/PSInSAR/normalized_amplitude_reshaped.npy')#loading normalized amplitudes reshaped as (-1, 37) aka (-1, INPUT_CHANNELS)\n",
    "y =  np.load('/content/drive/My Drive/PSInSAR/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4rm3IHE20Xy"
   },
   "outputs": [],
   "source": [
    "fOneHalfLoss = Fbeta_Loss(0.5)\n",
    "loss_function = fOneHalfLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAuRPbX7UXV5"
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "  ratio = params['ratio']\n",
    "  k = params['k']\n",
    "  nodes_phl = params['nodes_phl']\n",
    "  h_layers = params['h_layers']\n",
    "  p = params['p']\n",
    "  print('k: ', k, 'ratio: ', ratio, \"nodes_phl: \", nodes_phl, \"h_layers: \", h_layers, 'p: ', p)\n",
    "  INPUT_COUNT, train_sample, X_val, X_test, y_val, y_test = data_processing(X1, X2, y, k, ratio)\n",
    "  print(\"completed data processing\")\n",
    "  best_loss = train(INPUT_COUNT, train_sample, X_val, y_val, nodes_phl, h_layers, p)\n",
    "  \n",
    "  # Dictionary with information for evaluation\n",
    "  return {'loss': best_loss, 'params':params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_k6js0uGZEOo"
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "    'ratio': hp.choice('ratio',[1, 2, 3]),\n",
    "    'k': hp.choice('k', [1, 2, 4, 8, 16, 32, 64]),\n",
    "    'nodes_phl': hp.choice('nodes_phl',[32, 64, 128, 256, 512, 1024, 2048, 4096]),\n",
    "    'h_layers': hp.choice('h_layers',[1, 2, 3, 4, 5]),\n",
    "    'p': hp.choice('p',[0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]),\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4311,
     "status": "ok",
     "timestamp": 1580067933896,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "PmtTe-5eJkRy",
    "outputId": "2512ab11-3426-43c2-ba3b-7340b4b7a227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h_layers': 4, 'k': 32, 'nodes_phl': 512, 'p': 0.05, 'ratio': 1}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt.pyll.stochastic\n",
    "print(hyperopt.pyll.stochastic.sample(space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ep80R9HoAOua"
   },
   "outputs": [],
   "source": [
    "bayes_trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "E1js9qW9AAoC",
    "outputId": "3c8b9ff3-8066-4190-cc47-66502932fd7e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAX_EVALS = 500\n",
    "\n",
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOvXQJhm5zMWJNqoeS5wXWs",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "bayesian_opt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
