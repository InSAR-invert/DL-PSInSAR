{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exij64S29V0c"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from statistics import mean \n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1580195525822,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "ZOxyDIpg_6xJ",
    "outputId": "b8c12cf4-88a9-4571-8441-2411c9e5a666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#mounting drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-cLb_7eXf_Z"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(X1, X2):\n",
    "  INPUT_COUNT = X1.shape[1]\n",
    "  #next two line of codes take mean of one input-feature set and subtracts it from all of the elements and the returns the absolute values for normalized phase diff\n",
    "  X1 = X1 - np.transpose(X1.mean(axis=1).repeat(INPUT_COUNT).reshape(INPUT_COUNT, -1))\n",
    "  X1 = np.absolute(X1)\n",
    "  #next two line of codes take mean of one input-feature set and subtracts it from all of the elements and the returns the absolute values for normalized amplitudes\n",
    "  X2 = X2 - np.transpose(X2.mean(axis=1).repeat(INPUT_COUNT).reshape(INPUT_COUNT, -1))\n",
    "  X2 = np.absolute(X2)\n",
    "\n",
    "  X = np.append(X1, X2, axis=1)\n",
    "\n",
    "  del X1, X2\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKI778JsAQs9"
   },
   "outputs": [],
   "source": [
    "def data_processing(X, y, bagging=True, k=None, ratio=None, len_ensembled=None):\n",
    "\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)#creating test train split\n",
    "\n",
    "  del X, y\n",
    "\n",
    "  X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.2)#creating test validation split\n",
    "\n",
    "  INPUT_COUNT = X_test.shape[1]\n",
    "  train_samples = np.append(X_train, y_train.reshape(-1, 1), axis=1 )#appending input features and labels into one numpy array\n",
    "  del X_train\n",
    "  \n",
    "  train_samples_tensor = torch.tensor(train_samples)\n",
    "  X_val = torch.tensor(X_val)\n",
    "  X_test = torch.tensor(X_test)\n",
    "  y_val = torch.tensor(y_val)\n",
    "  y_test = torch.tensor(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  if bagging == False:\n",
    "    return INPUT_COUNT, train_samples_tensor, X_val, X_test, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "  nonPS = train_samples[y_train==0]#saving the example sets with label nonPS into one array\n",
    "  PS = train_samples[y_train==1]#saving the example sets with label PS into other array\n",
    "\n",
    "  del y_train\n",
    "\n",
    "  training_datasets_list=[]#intializing list to append all the different data-sets that will be created\n",
    "\n",
    "  for i in range(len_ensembled):\n",
    "\n",
    "    nonPS_sample = resample(nonPS, n_samples=int(k*ratio*len(PS)))#nonPS example-sets in the current data-set chosen with replacement, (k*ratio*(number of PS pixels)) in size\n",
    "    PS_sample = resample(PS, n_samples=int(k*len(PS)))#PS example-sets in the current data-set chosen with replacement, (k * (number of PS pixels)) in size\n",
    "\n",
    "    train_sample = np.append(nonPS_sample, PS_sample, axis=0)#appending PS and nonPS examples\n",
    "\n",
    "    np.random.shuffle(train_sample)#shuffling the appended array\n",
    "\n",
    "    training_datasets_list.append(train_sample)#appending the shuffled data-set to the list training_datasets_list\n",
    "\n",
    "  del nonPS, PS\n",
    "\n",
    "  training_datasets_list = torch.tensor(training_datasets_list)\n",
    "\n",
    "  \n",
    "\n",
    "  return INPUT_COUNT, training_datasets_list, X_val, X_test, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b8xtuwSgj4cB"
   },
   "outputs": [],
   "source": [
    "#class to instantize network object with \"h_layers\" hidden layers, \"nodes_phl\" nodes per hidden layer  and dropout layer with probability \"p\" \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, h_layers, nodes_phl, dropout_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h_layers = h_layers\n",
    "        \n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(INPUT_COUNT, nodes_phl),\n",
    "            nn.BatchNorm1d(nodes_phl),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden = nn.ModuleList()\n",
    "        for k in range(h_layers-1):\n",
    "            self.hidden.append(nn.Sequential(\n",
    "            nn.Linear(nodes_phl, nodes_phl),\n",
    "            nn.BatchNorm1d(nodes_phl),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "        \n",
    "        self.output = nn.Linear(nodes_phl, 1) if h_layers>0 else nn.Linear(INPUT_COUNT, 1)\n",
    "        \n",
    "        self.drop_layer = nn.Dropout(p=dropout_p)\n",
    "    \n",
    "    def forward(self, X):#forward propogation through network\n",
    "\n",
    "        if self.h_layers:\n",
    "          X = self.input(X)\n",
    "          X = self.drop_layer(X)\n",
    "        for layer in self.hidden:\n",
    "          X = layer(X)\n",
    "          X = self.drop_layer(X)\n",
    "        X = self.output(X)\n",
    "        return torch.sigmoid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAM6bhwz0a7p"
   },
   "outputs": [],
   "source": [
    "# function to calculate metrics for current batch\n",
    "def calc_metrics(y_pred, y_true, beta):\n",
    "  epsilon = 1e-7\n",
    "  '''rounded each element to either 0 for less than 0.5 or 1 for greater than 0.5'''\n",
    "  y_pred = torch.round(y_pred)\n",
    "\n",
    "  \n",
    "  tp = (y_true * y_pred).sum() # true positive\n",
    "  fp = ((1-y_true) * y_pred).sum() # false positive\n",
    "  fn = (y_true * (1-y_pred)).sum()# calculating false negative\n",
    "\n",
    "  precision = tp / (tp + fp + epsilon)\n",
    "  recall = tp / (tp + fn + epsilon )\n",
    "  fbeta = (1 + beta*beta)*tp / ((1 + beta*beta)*tp + (beta*beta)*fp + fn + epsilon)\n",
    "\n",
    "  return fbeta, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1sSwBb3Ej-f_"
   },
   "outputs": [],
   "source": [
    "#class to create F1_loss object instance\n",
    "class Fbeta_Loss(nn.Module):\n",
    "    \n",
    "    def __init__(self, beta=1, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, y_pred, y_true,):\n",
    "        \n",
    "        tp = (y_true * y_pred).sum()\n",
    "        fp = ((1 - y_true) * y_pred).sum()\n",
    "        fn = (y_true * (1 - y_pred)).sum()\n",
    "\n",
    "\n",
    "        fbeta = (1 + self.beta*self.beta)*tp / ((1 + self.beta*self.beta)*tp + (self.beta*self.beta)*fp + fn + self.epsilon)\n",
    "        return 1 - fbeta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npfdjOfDrUON"
   },
   "outputs": [],
   "source": [
    "#function to calculate loss and metrics\n",
    "def calc_loss_n_metrics(outputs, batch_y, beta):\n",
    "  outputs = outputs.view(-1)\n",
    "  loss = loss_function(outputs.cpu(), batch_y.cpu())\n",
    "  fbeta, precision, recall = calc_metrics(outputs.cpu(), batch_y.cpu(), beta)\n",
    "  return loss, fbeta, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r91QjRVcj-gF"
   },
   "outputs": [],
   "source": [
    "#function to pass batch through network with some additional arguments\n",
    "def fwd_pass(net, optimizer, batch_X, batch_y, train, beta):\n",
    "    \n",
    "    if train:\n",
    "      net.train()\n",
    "      net.zero_grad()\n",
    "      outputs = net(batch_X)\n",
    "      loss, fbeta, precision, recall = calc_loss_n_metrics(outputs, batch_y, beta)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "    else:\n",
    "      net.eval()\n",
    "      with torch.no_grad():\n",
    "        outputs = net(batch_X)\n",
    "      loss, fbeta, precision, recall = calc_loss_n_metrics(outputs, batch_y, beta)\n",
    "    \n",
    "    \n",
    "\n",
    "    return loss, fbeta, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Tr4OmCdyPiR"
   },
   "outputs": [],
   "source": [
    "# completing one epoch through the given dataset\n",
    "def one_epoch(net, optimizer, X, y, BATCH_SIZE, beta, train=False):\n",
    "  LOSS = []\n",
    "  Fbeta = []\n",
    "  PRECISION = []\n",
    "  RECALL = []\n",
    "  for i in range(0, len(X), BATCH_SIZE):\n",
    "    batch_X = X[i:i+BATCH_SIZE].view(-1, INPUT_COUNT)\n",
    "    batch_y = y[i:i+BATCH_SIZE].view(-1)\n",
    "    \n",
    "    batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "\n",
    "    loss, fbeta, precision, recall = fwd_pass(net, optimizer, batch_X, batch_y, train, beta)\n",
    "    LOSS.append(loss)\n",
    "    Fbeta.append(fbeta)\n",
    "    PRECISION.append(precision)\n",
    "    RECALL.append(recall)\n",
    "  LOSS = torch.mean(torch.stack(LOSS))\n",
    "  Fbeta = torch.mean(torch.stack(Fbeta))\n",
    "  PRECISION = torch.mean(torch.stack(PRECISION))\n",
    "  RECALL = torch.mean(torch.stack(RECALL))\n",
    "\n",
    "  return LOSS, Fbeta, PRECISION, RECALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltniYhAsjEBQ"
   },
   "outputs": [],
   "source": [
    "#function to train the network and save the list of best models\n",
    "def train(training_datasets_list, metric_to_optimimze, number_of_training_datasets_to_train_on = 1, beta=1, threshold=200, h_layers=1, nodes_phl=1024, p=0.0):\n",
    "    \n",
    "    BATCH_SIZE = 32768\n",
    "    net_list=[]\n",
    "\n",
    "    for i in tqdm(range(number_of_training_datasets_to_train_on)):\n",
    "\n",
    "      #instantizing network with 1 hidden layer, 1024 nodes per hidden layer and 0.0 dropout probability as obtained from hyper-parameter tuning\n",
    "      net = Net(h_layers, nodes_phl, p).cuda()\n",
    "\n",
    "      #intializing optimimzer instance\n",
    "      optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "      best_val_fbeta=0\n",
    "      best_val_precision=0\n",
    "      best_val_recall=0\n",
    "\n",
    "     \n",
    "      best_net = Net(h_layers, nodes_phl, p).cuda()\n",
    "      counter = 0\n",
    "\n",
    "\n",
    "      #setting threshold epochs to terminate the training after which no improvement in the chosen validation metric is observed \n",
    "      THRESHOLD = threshold\n",
    "\n",
    "      while(True):\n",
    "        \n",
    "        epoch_training_loss, epoch_training_fbeta, epoch_training_precision, epoch_training_recall = one_epoch(net, optimizer, training_datasets_list[i][:,0:-1], training_datasets_list[i][:,-1], BATCH_SIZE, beta, True)\n",
    "        \n",
    "        epoch_val_loss, epoch_val_fbeta, epoch_val_precision, epoch_val_recall = one_epoch(net, optimizer, X_val, y_val, BATCH_SIZE, beta, False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def update_counter(net, counter, epoch_val_metric, best_val_metric):\n",
    "\n",
    "          is_best_metric = epoch_val_metric > best_val_metric\n",
    "          best_val_metric = max(epoch_val_metric, best_val_metric)\n",
    "          \n",
    "\n",
    "          if is_best_metric:\n",
    "            counter=0\n",
    "            nonlocal best_net\n",
    "            best_net.load_state_dict(net.state_dict())\n",
    "             \n",
    "          \n",
    "          counter += 1\n",
    "\n",
    "          return counter, best_val_metric\n",
    "\n",
    "        \n",
    "        if metric_to_optimimze=='fbeta':\n",
    "          counter, best_val_fbeta = update_counter(net, counter, epoch_val_fbeta, best_val_fbeta)\n",
    "\n",
    "        if metric_to_optimimze=='precision':\n",
    "          counter, best_val_precision = update_counter(net, counter, epoch_val_precision, best_val_precision)\n",
    "\n",
    "        if metric_to_optimimze=='recall':\n",
    "          counter, best_val_recall = update_counter(net, counter, epoch_val_recall, best_val_recall)\n",
    "\n",
    "    \n",
    "        if(counter==THRESHOLD):\n",
    "          net_list.append(best_net)\n",
    "          break\n",
    "\n",
    "    return net_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3Sji0EpQWGq"
   },
   "outputs": [],
   "source": [
    "#function to test our list of models on a test set which it has never seen before\n",
    "def predict(combined_net, X_test, y_test, beta=1):\n",
    "  combined_predictions_list=[]\n",
    "  length = len(X_test)\n",
    "  length_y = len(y_test)\n",
    "  for net in combined_net:\n",
    "    X_test=X_test.cuda()\n",
    "    y_test=y_test.cuda()\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "      predictions = net(X_test)\n",
    "    combined_predictions_list.append(predictions.cpu())\n",
    "  for predictions in combined_predictions_list:\n",
    "    torch.unsqueeze(predictions, 0)\n",
    "  combined_predictions_tensor=torch.cat(combined_predictions_list)\n",
    "  combined_predictions_tensor=combined_predictions_tensor.view(len(combined_predictions_list), length, 1)\n",
    "  combined_predictions_tensor =  torch.mean(combined_predictions_tensor, 0)\n",
    "  loss, fbeta, precision, recall = calc_loss_n_metrics(combined_predictions_tensor, y_test, beta)\n",
    "  return combined_predictions_tensor, fbeta, precision, recall\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGHb6Rc8o5Xt"
   },
   "outputs": [],
   "source": [
    "#all hyperparameters obtained from hyperparameter tuning\n",
    "ratio = 2#ratio of majority to minority class in the new data-set s created\n",
    "k = 12.326192312106985#size of each data set would be k*(ratio+1)*total number of PS pixels in train data-set created\n",
    "len_ensembled = 15#create 15 data-sets 15 being an arbitary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDWFlkayo5Xh"
   },
   "outputs": [],
   "source": [
    "#generating a list of data-set using bagging\n",
    "GENERATE = False\n",
    "if GENERATE:\n",
    "  X1 =  np.load('/content/drive/My Drive/PSInSAR/normalized_phase_diff.npy')#loading normalized phase diff interferograms reshaped as (-1, 37) aka (-1, INPUT_CHANNELS)\n",
    "  X2 = np.load('/content/drive/My Drive/PSInSAR/normalized_amplitude_reshaped.npy')#loading normalized amplitudes reshaped as (-1, 37) aka (-1, INPUT_CHANNELS)\n",
    "  y =  np.load('/content/drive/My Drive/PSInSAR/labels.npy')\n",
    "  X = data_preprocessing(X1, X2)\n",
    "  del X1, X2\n",
    "  INPUT_COUNT, training_datasets_list, X_val, X_test, y_val, y_test = data_processing(X, y, True, k, ratio, len_ensembled)\n",
    "  del X, y\n",
    "  torch.save({    \n",
    "    'INPUT_COUNT': INPUT_COUNT,\n",
    "    'training_datasets_list': training_datasets_list,\n",
    "    'X_val': X_val,\n",
    "    'X_test': X_test,\n",
    "    'y_val':y_val,\n",
    "    'y_test': y_test\n",
    "            }, \"/content/drive/My Drive/PSInSAR/data_splits.pt\" )\n",
    "else:\n",
    "  data_splits = torch.load(\"/content/drive/My Drive/PSInSAR/data_splits.pt\")\n",
    "  INPUT_COUNT = data_splits['INPUT_COUNT']\n",
    "  del data_splits['INPUT_COUNT']\n",
    "  training_datasets_list = data_splits['training_datasets_list']\n",
    "  del data_splits['training_datasets_list']\n",
    "  X_val = data_splits['X_val']\n",
    "  del data_splits['X_val']\n",
    "  X_test = data_splits['X_test']\n",
    "  del data_splits['X_test']\n",
    "  y_val = data_splits['y_val']\n",
    "  del data_splits['y_val']\n",
    "  y_test = data_splits['y_test']\n",
    "  del  data_splits['y_test']\n",
    "  del data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_b9Qumz6f5hZ"
   },
   "outputs": [],
   "source": [
    "#instantizing loss_function \n",
    "f1_loss = Fbeta_Loss(1)\n",
    "f2_loss = Fbeta_Loss(2)\n",
    "f3_loss = Fbeta_Loss(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XooHEsehA6j"
   },
   "outputs": [],
   "source": [
    "#instantizing loss_function \n",
    "fThreeFourthLoss = Fbeta_Loss(0.75)\n",
    "fOneHalfLoss = Fbeta_Loss(0.5)\n",
    "fOneThirdLoss = Fbeta_Loss(0.33)\n",
    "fOneFourthLoss = Fbeta_Loss(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50721,
     "status": "ok",
     "timestamp": 1580022716839,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "x0WwXYcSDvTx",
    "outputId": "6b4e58c2-3379-40b4-909a-456819295c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "torch.Size([15, 941880, 75])\n",
      "torch.Size([954552, 74]) torch.Size([954552])\n",
      "torch.Size([238639, 74]) torch.Size([238639])\n"
     ]
    }
   ],
   "source": [
    "print(INPUT_COUNT)\n",
    "print(training_datasets_list.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1346655,
     "status": "ok",
     "timestamp": 1580055612836,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "YuCFXhuzjaZ8",
    "outputId": "4b7dac10-25fa-40d5-bf2d-587d4c649089"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [22:23<00:00, 1343.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5056) tensor(0.1051) tensor(0.6637)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bagging fOneFourthscore fOneFourthloss threshold=200 data_size_len=1\n",
    "loss_function = fOneFourthLoss\n",
    "net_list_fOneFourth_1 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=1, beta=0.25)\n",
    "_, fOneFourth, precision, recall = predict(net_list_fOneFourth_1, X_test, y_test, beta = 0.25)\n",
    "print(fOneFourth, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2331394,
     "status": "ok",
     "timestamp": 1580056597581,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "s-h-8ISBiaCB",
    "outputId": "92c9cd30-55e8-4cd5-ced4-db7d0b3ae486"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [16:24<00:00, 984.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4641) tensor(0.1423) tensor(0.6158)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bagging fOneThirdscore fOneThirdloss threshold=200 data_size_len=1\n",
    "loss_function = fOneThirdLoss\n",
    "net_list_fOneThird_1 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=1, beta=0.33)\n",
    "_, fOneThird, precision, recall = predict(net_list_fOneThird_1, X_test, y_test, beta = 0.33)\n",
    "print(fOneThird, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3097717,
     "status": "ok",
     "timestamp": 1580057363908,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "QBWQcZa7iH3W",
    "outputId": "18c8a498-607f-469e-8a9f-1b2ef81c961c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [12:46<00:00, 766.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4022) tensor(0.1884) tensor(0.5615)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bagging fOneHalfscore fOneHalfloss threshold=200 data_size_len=1\n",
    "loss_function = fOneHalfLoss\n",
    "net_list_fOneHalf_1 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=1, beta=0.5)\n",
    "_, fOneHalf, precision, recall = predict(net_list_fOneHalf_1, X_test, y_test, beta = 0.5)\n",
    "print(fOneHalf, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3683861,
     "status": "ok",
     "timestamp": 1580057950055,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "aNjmrka3hw_q",
    "outputId": "434b1e6c-61e8-4949-cba9-5426757f6313"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:45<00:00, 585.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3482) tensor(0.2228) tensor(0.5096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bagging fThreeFourthscore fThreeFourthloss threshold=200 data_size_len=1\n",
    "loss_function = fThreeFourthLoss\n",
    "net_list_fThreeFourth_1 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=1, beta=0.75)\n",
    "_, fThreeFourth, precision, recall = predict(net_list_fThreeFourth_1, X_test, y_test, beta = 0.75)\n",
    "print(fThreeFourth, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 523431,
     "status": "ok",
     "timestamp": 1580023190773,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "vuyKZzAjXJSl",
    "outputId": "5628e435-64d8-4285-f053-c80ce6610c0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [07:53<00:00, 473.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3188) tensor(0.2497) tensor(0.4409)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bagging f1score f1loss threshold=200 data_size_len=1\n",
    "loss_function = f1_loss\n",
    "net_list_f1_1 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=1, beta=1)\n",
    "_, f1, precision, recall = predict(net_list_f1_1, X_test, y_test, beta = 1)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1153591,
     "status": "ok",
     "timestamp": 1580023822264,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "_s3zICO8ZnEP",
    "outputId": "3a4c6d1f-04bd-4988-c7b7-8bf1be48c750"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [10:31<00:00, 631.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3063) tensor(0.2915) tensor(0.3842)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bagging f2score f1loss threshold=200 data_size_len=1\n",
    "loss_function = f2_loss\n",
    "net_list_f2_1 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=1, beta=2)\n",
    "_, f2, precision, recall = predict(net_list_f2_1, X_test, y_test, beta=2)\n",
    "print(f2, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1560048,
     "status": "ok",
     "timestamp": 1580024229447,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "L44MIChIXZAe",
    "outputId": "9d35f96e-c670-4c27-f2ef-5562e2a93fc2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [06:47<00:00, 407.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3208) tensor(0.3188) tensor(0.3395)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#bagging f3score f1loss threshold=200 data_size_len=1\n",
    "loss_function = f3_loss\n",
    "net_list_f3_1 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=1, beta=3)\n",
    "_, f3, precision, recall = predict(net_list_f3_1, X_test, y_test, beta=3)\n",
    "print(f3, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7836223,
     "status": "ok",
     "timestamp": 1580128834790,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "YKJOvnvxBLXd",
    "outputId": "376ffaf0-4e1d-4a52-feb0-ffed75ba38e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [4:34:25<00:00, 1051.32s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5327) tensor(0.1227) tensor(0.6733)\n",
      "tensor(0.5327) tensor(0.1227) tensor(0.6733)\n"
     ]
    }
   ],
   "source": [
    "#bagging fOneFourthscore fONeFourthloss threshold=200 \n",
    "loss_function = fOneFourthLoss\n",
    "net_list_fbeta_15 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=15, beta=0.25)\n",
    "state_dict_list=[]\n",
    "for net in net_list_fbeta_15:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "torch.save(state_dict_list, \"/content/drive/My Drive/PSInSAR/net_list_fOneFourth_15.pt\" )\n",
    "_, fbeta, precision, recall = predict(net_list_fbeta_15, X_test, y_test, beta=0.25)\n",
    "print(fbeta, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2333,
     "status": "ok",
     "timestamp": 1580206793650,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "B0v48jmuOMER",
    "outputId": "882600a4-2927-45a8-fc40-dbc0bee9fa1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4235) tensor(0.2144) tensor(0.5599)\n"
     ]
    }
   ],
   "source": [
    "#bagging fOneHalfscore fOneHalfLoss threshold=200 \n",
    "loss_function = fOneHalfLoss\n",
    "net_list_fbeta_15 = train(training_datasets_list, 'fbeta', number_of_training_datasets_to_train_on=15, beta=0.5)\n",
    "state_dict_list=[]\n",
    "for net in net_list_fbeta_15:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "torch.save(state_dict_list, \"/content/drive/My Drive/PSInSAR/net_list_fOneHalf_15.pt\" )\n",
    "_, fbeta, precision, recall = predict(net_list_fbeta_15, X_test, y_test, beta=0.5)\n",
    "print(fbeta, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32rtOI8UiPEi"
   },
   "outputs": [],
   "source": [
    "#bagging precision f1loss threshold=200 \n",
    "net_list_precision_15 = train(training_datasets_list, 'precision', number_of_training_datasets_to_train_on=15)\n",
    "state_dict_list=[]\n",
    "for net in net_list_precision_15:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "torch.save(state_dict_list, \"/content/drive/My Drive/PSInSAR/net_list_precision_15.pt\" )\n",
    "_, f1, precision, recall = predict(net_list_precision_15, X_test, y_test)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9522898,
     "status": "ok",
     "timestamp": 1579321325551,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "1iwEuZIDfIrO",
    "outputId": "437d1791-b751-48cc-e342-a658af77b059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [2:38:41<00:00, 678.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3514) tensor(0.2877) tensor(0.4513)\n"
     ]
    }
   ],
   "source": [
    "#bagging f1score f1loss threshold=200 \n",
    "net_list_f1_15 = train(training_datasets_list, 'f1', number_of_training_datasets_to_train_on=15)\n",
    "state_dict_list=[]\n",
    "for net in net_list_f1_15:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "torch.save(state_dict_list, \"/content/drive/My Drive/PSInSAR/net_list_f1_15.pt\" )\n",
    "_, f1, precision, recall = predict(net_list_f1_15, X_test, y_test)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2131145,
     "status": "ok",
     "timestamp": 1579323456684,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "lMXYoHT6fJuP",
    "outputId": "11e3dc8c-d77e-46d3-9acf-796360a34125"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [35:29<00:00, 141.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0104) tensor(0.0052) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#bagging recall f1loss threshold=200 \n",
    "net_list_recall_15 = train(training_datasets_list, 'recall', number_of_training_datasets_to_train_on=15)\n",
    "for net in net_list_recall_15:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "torch.save(state_dict_list, \"/content/drive/My Drive/PSInSAR/net_list_recall_15.pt\" )\n",
    "_, f1, precision, recall = predict(net_list_recall_15, X_test, y_test)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13781408,
     "status": "ok",
     "timestamp": 1579989982830,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "YQHqHfso6b_d",
    "outputId": "52f1cacb-ace5-48d5-bffa-27cda4bbc57a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [3:49:38<00:00, 853.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3479) tensor(0.2985) tensor(0.4169)\n"
     ]
    }
   ],
   "source": [
    "#Bagging precision f1loss threshold=500\n",
    "loss_function = f1_loss\n",
    "net_list_precision_15 = train(training_datasets_list, 'precision', number_of_training_datasets_to_train_on=15, threshold=500)\n",
    "state_dict_list=[]\n",
    "for net in net_list_precision_15:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "torch.save(state_dict_list, \"/content/drive/My Drive/PSInSAR/net_list_precision_15_thr_500.pt\" )\n",
    "_, f1, precision, recall = predict(net_list_precision_15, X_test, y_test)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3972083,
     "status": "ok",
     "timestamp": 1579994345960,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "kFCkn6HJneg-",
    "outputId": "1761ce06-871c-4a5e-d9d8-e18e2bc2e356"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [4:57:55<00:00, 1244.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3508) tensor(0.2869) tensor(0.4513)\n"
     ]
    }
   ],
   "source": [
    "#Bagging f1score f1loss threshold=500\n",
    "net_list_f1_15 = train(training_datasets_list, 'f1', number_of_training_datasets_to_train_on=15, threshold=500)\n",
    "state_dict_list=[]\n",
    "for net in net_list_f1_15:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "torch.save(state_dict_list, \"/content/drive/My Drive/PSInSAR/net_list_f1_15_thr_500.pt\" )\n",
    "_, f1, precision, recall = predict(net_list_f1_15, X_test, y_test)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyebWccloMjq"
   },
   "outputs": [],
   "source": [
    "#original dataset\n",
    "X1 =  np.load('/content/drive/My Drive/PSInSAR/normalized_phase_diff.npy')#loading normalized phase diff interferograms reshaped as (-1, 37) aka (-1, INPUT_CHANNELS)\n",
    "X2 = np.load('/content/drive/My Drive/PSInSAR/normalized_amplitude_reshaped.npy')#loading normalized amplitudes reshaped as (-1, 37) aka (-1, INPUT_CHANNELS)\n",
    "y =  np.load('/content/drive/My Drive/PSInSAR/labels.npy')\n",
    "X = data_preprocessing(X1, X2)\n",
    "del X1, X2\n",
    "INPUT_COUNT, training_datasets_list_no_baggaging, X_val, X_test, y_val, y_test = data_processing(X, y, False)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1113858,
     "status": "ok",
     "timestamp": 1580026752933,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "XJ9pyyKmi5oK",
    "outputId": "afde7f99-09c2-41b3-95e7-e108ebaf3a1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [35:23<00:00, 2123.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0500) tensor(0.5000) tensor(0.0263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#noBagging precision f1loss threshold=200\n",
    "loss_function = f1_loss\n",
    "net_list_precision_1_no_b = train([training_datasets_list_no_baggaging], 'precision')\n",
    "state_dict_list=[]\n",
    "for net in net_list_precision_1_no_b:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "_, f1, precision, recall = predict(net_list_precision_1_no_b, X_test, y_test)\n",
    "print(f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 884416,
     "status": "ok",
     "timestamp": 1580027637338,
     "user": {
      "displayName": "Suraj Kumar",
      "photoUrl": "",
      "userId": "05344270211403303935"
     },
     "user_tz": -330
    },
    "id": "khqqyW0P_XJN",
    "outputId": "238bd758-e6a9-4444-d5cd-998965d49482"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [14:43<00:00, 883.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2563) tensor(0.2269) tensor(0.2945)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#noBagging f1score f1loss threshold=200\n",
    "loss_function = f1_loss\n",
    "net_list_f1_1_no_b = train([training_datasets_list_no_baggaging], 'fbeta')\n",
    "state_dict_list=[]\n",
    "for net in net_list_f1_1_no_b:\n",
    "  state_dict_list.append(net.state_dict())\n",
    "_, f1, precision, recall = predict(net_list_f1_1_no_b, X_test, y_test)\n",
    "print(f1, precision, recall)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOtJuHiBGR2hpAkwKJOpfT9",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "training_ensemble_of_tuned_ann.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
