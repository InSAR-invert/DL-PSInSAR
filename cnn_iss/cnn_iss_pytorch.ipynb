{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"cnn_iss_pytorch.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"a8VjOrXLj-fs","colab_type":"code","colab":{}},"source":["#importing libraries\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from statistics import mean \n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvMSfi3xkMax","colab_type":"code","colab":{}},"source":["#mounting my drive\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x40TeKkuYhCQ","colab_type":"code","colab":{}},"source":["#splitting and shuffling data into training and validation splits\n","SPLIT = False\n","\n","PATH = F\"/content/drive/My Drive/data_split_cnn_iss.pt\"\n","\n","\n","if SPLIT:\n","  #loading data as numpy arrays\n","  X = np.load('/content/drive/My Drive/diff_intf/X_img_size_50.npy')\n","  y = np.load('/content/drive/My Drive/diff_intf/y_img_size_50.npy')\n","  #reading input channels and input image size of one feature-set\n","  INPUT_CHANNELS = X.shape[1]\n","  INPUT_IMG_SIZE = X.shape[2]\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","  #converting numpy arrays to tensors\n","  X_train = torch.Tensor([i for i in X_train]).view(-1, INPUT_CHANNELS, INPUT_IMG_SIZE, INPUT_IMG_SIZE)\n","  X_test = torch.Tensor([i for i in X_test]).view(-1, INPUT_CHANNELS, INPUT_IMG_SIZE, INPUT_IMG_SIZE)\n","  y_train = torch.Tensor([i for i in y_train])\n","  y_test = torch.Tensor([i for i in y_test])\n","  #Normalaizing X \n","  X_train = X_train/255.0\n","  X_test = X_test/255.0\n","  \n","  \n","  torch.save({\n","            'X_train': X_train,\n","            'X_test': X_test,\n","            'y_train': y_train,\n","            'y_test': y_test\n","        }, PATH)\n","  \n","else:\n","  \n","  checkpoint = torch.load(PATH)\n","  \n","  X_train = checkpoint['X_train']\n","  X_test = checkpoint['X_test']\n","  y_train = checkpoint['y_train']\n","  y_test = checkpoint['y_test']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPqvZEqGj-fw","colab_type":"code","colab":{}},"source":["#class to instantnize network object\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.features1 = nn.Sequential(\n","            nn.Conv2d(INPUT_CHANNELS, 16, 1, 1, 1),# args(INPUT_CHANNELS, output_channels, kernerl_size, strides, padding)\n","            nn.BatchNorm2d(16),\n","            nn.ReLU()\n","        )\n","        \n","        self.features2 = nn.Sequential(\n","            nn.Conv2d(16, 16, 1, 1, 1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU()\n","        )\n","        \n","        self.features3 = nn.Sequential(\n","            nn.Conv2d(16, 32, 1, 1, 1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU()\n","        )\n","        \n","        self.features4 = nn.Sequential(\n","            nn.Conv2d(32, 64, 1, 1, 1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU()\n","        )\n","\n","        \n","        #below 3 lines of code determines the input nodes to the fully connected layer\n","        X = torch.randn(INPUT_CHANNELS, INPUT_IMG_SIZE, INPUT_IMG_SIZE).view(-1, INPUT_CHANNELS, INPUT_IMG_SIZE, INPUT_IMG_SIZE)\n","        self._to_linear = None\n","        self.conv(X)\n","        \n","        self.drop_layer = nn.Dropout(p=0.25)\n","        self.fc1 = nn.Linear(self._to_linear, INPUT_IMG_SIZE*INPUT_IMG_SIZE)\n","        \n","        \n","        \n","        \n","    def conv(self, X):# forward propogation through con nets only\n","        X = self.features1(X)\n","        X = self.features2(X)\n","        X = self.features3(X)\n","        X = self.features4(X)\n","        if self._to_linear is None:\n","            self._to_linear = X[0].shape[0]*X[0].shape[1]*X[0].shape[2]\n","            \n","        return X\n","    \n","    \n","    def forward(self, X):#forward propogation through complete network\n","        X = self.conv(X)\n","        X = X.view(-1, self._to_linear)\n","        X = self.drop_layer(X)\n","        X = self.fc1(X)\n","        \n","        return torch.sigmoid(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aAA9XCY3j-fy","colab_type":"code","colab":{}},"source":["#instantizing network\n","net = Net()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nAM6bhwz0a7p","colab_type":"code","colab":{}},"source":["# function to calculate metrics for current batch\n","def calc_metrics(y_pred, y_true):\n","  epsilon = 1e-7\n","  '''rounded each element to either 0 for less than 0.5 or 1 for greater than 0.5'''\n","  y_pred = torch.round(y_pred)\n","\n","  \n","  tp = (y_true * y_pred).sum() # true positive\n","  fp = ((1-y_true) * y_pred).sum() # false positive\n","  fn = (y_true * (1-y_pred)).sum()# calculating false negative\n","\n","  precision = tp / (tp + fp + epsilon)\n","  recall = tp / (tp + fn + epsilon )\n","  f1 = 2*tp / (2*tp + fp + fn + epsilon)\n","\n","  return f1, precision, recall\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6-h9FIirLae","colab_type":"code","colab":{}},"source":["#function to calculate a class_weight tensor of shape (batchsize, 1) for current batch\n","def calc_class_weights_tensor(batch_y):\n","    '''reshape y '''\n","    y_reshaped = batch_y.view(-1)\n","\n","    '''calculate ratio of class1 to class2'''\n","    ratio = calc_ratio(y_reshaped)\n","    \n","    ''' generate class_weight_tensor for y with ratio'''\n","    class_weight_tensor = calc_class_weight_tensor(y_reshaped, ratio)\n","    \n","   \n","    return class_weight_tensor\n","\n","\n","def calc_ratio(y):\n","  matches  = [torch.round(i)==1 for i in y]\n","  class1 = matches.count(True)\n","  class2 = y.shape[0] - class1\n","  ratio = torch.tensor(class2/class1)\n","  return ratio\n","  \n","\n","def calc_class_weight_tensor(y, ratio):\n","  t = torch.tensor([0, 1])\n","  t = t.repeat(y.shape[0]).view(-1, 2)\n","  t = (t==y) + 0\n","  t = t*(ratio-1)\n","  t += 1\n","  return (t/ratio)[:, :1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTjSaHf5rCNX","colab_type":"code","colab":{}},"source":["#function to calculate class_weights for current batch\n","def calc_class_weights(batch_y):\n","    '''reshape y '''\n","    y_reshaped = batch_y.view(-1)\n","\n","    '''calculate ratio of class1 to class2'''\n","    ratio = calc_ratio(y_reshaped)\n","    \n","    \n","    return ratio\n","\n","\n","def calc_ratio(y):\n","  matches  = [torch.round(i)==1 for i in y]\n","  class1 = matches.count(True)\n","  class2 = y.shape[0] - class1\n","  ratio = torch.tensor(class1/class2)\n","  return ratio\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zX31Laeaq8_p","colab_type":"code","colab":{}},"source":["#class to create Macro_soft_double_F1_loss object instance\n","class Macro_Double_Soft_F1_Loss(nn.Module):\n","    \n","    def __init__(self, epsilon=1e-16):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        self.class_weight = calc_class_weights(y_train)\n","        \n","    def forward(self, y_pred, y_true):\n","        \n","        tp = (y_true * y_pred).sum()\n","        tn = ((1 - y_true) * (1 - y_pred)).sum()\n","        fp = ((1 - y_true) * y_pred).sum()\n","        fn = (y_true * (1 - y_pred)).sum()\n","\n","\n","        f1_class1 = 2* tp / (2*tp + fp + fn + self.epsilon)\n","        f1_class1 = f1_class1.clamp(min=self.epsilon, max=1-self.epsilon)\n","\n","        f1_class0 = 2* tn / (2*tn + fp + fn + self.epsilon)\n","        f1_class0 = f1_class0.clamp(min=self.epsilon, max=1-self.epsilon)\n","\n","        cost_class0 = 1 - f1_class0\n","        cost_class1 = 1 - f1_class1\n","        \n","        cost = (self.class_weight*cost_class0 + cost_class1)/(1+ self.class_weight)\n","\n","        return cost\n","\n","macro_double_soft_f1_loss = Macro_Double_Soft_F1_Loss()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sSwBb3Ej-f_","colab_type":"code","colab":{}},"source":["#class to create F1_loss object instance\n","class F1_Loss(nn.Module):\n","    \n","    def __init__(self, epsilon=1e-7):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        \n","    def forward(self, y_pred, y_true,):\n","        \n","        tp = (y_true * y_pred).sum()\n","        fp = ((1 - y_true) * y_pred).sum()\n","        fn = (y_true * (1 - y_pred)).sum()\n","\n","\n","        f1 = 2*tp / (2*tp + fp + fn + self.epsilon)\n","        return 1 - f1\n","\n","f1_loss = F1_Loss()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"32rSpRrFrlEn","colab_type":"code","colab":{}},"source":["#instantizing bce loss\n","bceloss = nn.BCELoss(reduction = 'none')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AolMDusqrhdP","colab_type":"code","colab":{}},"source":["#function to calculate weighted bce loss \n","def calc_loss_n_acc(outputs, batch_y, class_weights):\n","  outputs = outputs.view(-1)\n","  batch_y = batch_y.view(-1)\n","  loss = bceloss(outputs, batch_y)\n","  loss_class_weighted = loss * class_weights\n","  loss_class_weighted = loss_class_weighted.mean()\n","  return loss_class_weighted"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_b9Qumz6f5hZ","colab_type":"code","colab":{}},"source":["#instantizing f1 loss \n","loss_function = f1_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"npfdjOfDrUON","colab_type":"code","colab":{}},"source":["#function to calculate loss and metrics\n","def calc_loss_n_metrics(outputs, batch_y):\n","  outputs = outputs.view(-1)\n","  loss = loss_function(outputs, batch_y)\n","  f1, precision, recall = calc_metrics(outputs, batch_y)\n","  return loss, f1, precision, recall"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r91QjRVcj-gF","colab_type":"code","colab":{}},"source":["#function to pass batch through network with some additional arguments\n","def fwd_pass(batch_X, batch_y, train):\n","    \n","    if train:\n","      net.train()\n","      net.zero_grad()\n","      outputs = net(batch_X)\n","      loss, f1, precision, recall = calc_loss_n_metrics(outputs, batch_y)\n","      loss.backward()\n","      optimizer.step()  \n","    \n","    else:\n","      net.eval()\n","      with torch.no_grad():\n","        outputs = net(batch_X)\n","      loss, f1, precision, recall = calc_loss_n_metrics(outputs, batch_y)\n","    \n","    \n","\n","    return loss, f1, precision, recall"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Tr4OmCdyPiR","colab_type":"code","colab":{}},"source":["# completing one epoch through the given dataset\n","def one_epoch(X, y, BATCH_SIZE, train=False):\n","  LOSS = []\n","  F1 = []\n","  PRECISION = []\n","  RECALL = []\n","  for i in tqdm(range(0, len(X), BATCH_SIZE)):\n","    batch_X = X[i:i+BATCH_SIZE].view(-1, INPUT_CHANNELS, INPUT_IMG_SIZE, INPUT_IMG_SIZE)\n","    batch_y = y[i:i+BATCH_SIZE].view(-1)\n","    \n","    loss, f1, precision, recall = fwd_pass(batch_X, batch_y, train)\n","    LOSS.append(loss)\n","    F1.append(f1)\n","    PRECISION.append(precision)\n","    RECALL.append(recall)\n","  LOSS=torch.mean(torch.stack(LOSS))\n","  F1=torch.mean(torch.stack(F1))\n","  PRECISION=torch.mean(torch.stack(PRECISION))\n","  RECALL=torch.mean(torch.stack(RECALL))\n","\n","  return LOSS, F1, PRECISION, RECALL\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_E0Inyhj-f9","colab_type":"code","colab":{}},"source":["#intializing optimimzer instance\n","optimizer = optim.Adam(net.parameters())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eKcpV0T2mQrN","colab_type":"code","colab":{}},"source":["#instantizing TRAINING and VALIDATION lists to save metrics\n","TRAINING=[]\n","VALIDATION=[]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltniYhAsjEBQ","colab_type":"code","colab":{}},"source":["#function to train the network\n","def train(net, best_val_loss):\n","    BATCH_SIZE = 8\n","    EPOCHS = 20\n","\n","    for epoch in range(EPOCHS):\n","        \n","        epoch_training_loss, epoch_training_f1, epoch_training_precision, epoch_training_recall = one_epoch(X_train, y_train, BATCH_SIZE, True)\n","        \n","        TRAINING.append([epoch_training_loss.item(), epoch_training_f1.item(), epoch_training_precision.item(), epoch_training_recall.item()])\n","        print('training_loss: ', epoch_training_loss)    \n","        print('training_f1: ', epoch_training_f1)\n","        print('training_precision: ', epoch_training_precision)\n","        print('training_recall: ', epoch_training_recall)\n","        \n","        epoch_val_loss, epoch_val_f1, epoch_val_precision, epoch_val_recall = one_epoch(X_test, y_test, BATCH_SIZE, False)\n","        \n","        VALIDATION.append([epoch_val_loss.item(), epoch_val_f1.item(), epoch_val_precision.item(), epoch_val_recall.item()])\n","        print('val_loss: ', epoch_val_loss)\n","        print('val_f1: ', epoch_val_f1)\n","        print('val_precision: ', epoch_val_precision)\n","        print('val_recall: ', epoch_val_recall)\n","\n","        \n","        if not best_val_loss:\n","          best_val_loss = epoch_val_loss + 1\n","\n","        is_best = epoch_val_loss < best_val_loss\n","        best_val_loss = min(epoch_val_loss, best_val_loss)\n","\n","        if is_best:\n","          print('Saving latest iteration')\n","          torch.save({\n","            'model_state_dict': net.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'best_val_loss': best_val_loss\n","        }, F\"/content/drive/My Drive/cnn_iss.pt\" )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHyppwwajiiI","colab_type":"code","colab":{}},"source":["#load saved model\n","PATH = F\"/content/drive/My Drive/cnn_iss.pt\" \n","checkpoint = torch.load(PATH)\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","net.load_state_dict(checkpoint['model_state_dict'])\n","best_val_loss=checkpoint['best_val_loss']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8VrpdjLIVlL","colab_type":"code","colab":{}},"source":["#training network\n","train(net, None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QRl9LzYt59a","colab_type":"code","colab":{}},"source":["#save training and validation metrics\n","torch.save({\n","            'training_metric': TRAINING,\n","            'validation_metric': VALIDATION,\n","        }, F\"/content/drive/My Drive/cnn_iss.pt\" )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceZdJd7Wv3wl","colab_type":"code","colab":{}},"source":["#load saved training and validation metrics\n","PATH = F\"/content/drive/My Drive/cnn_iss.pt.pt\" \n","checkpoint = torch.load(PATH)\n","TRAINING = checkpoint['training_metric']\n","VALIDATION = checkpoint['validation_metric']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9p6QoMtVMKB","colab_type":"code","colab":{}},"source":["#function to calculate accuracy of the model\n","def calc_acc(X, y):\n","  accs=[]\n","  for i in tqdm(range(0, len(X), 100)):\n","    batch_X = X[i:i+100].view(-1, 37, 50, 50)\n","    net.eval()\n","    batch_y = y[i:i+100]\n","    with torch.no_grad():\n","      o = net(batch_X)\n","    o = torch.round(o)\n","    matches  = [i==j for i, j in zip(o.view(-1), batch_y.view(-1))]\n","    acc = matches.count(True)/len(matches)\n","    accs.append(acc)\n","  return mean(accs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ym5t70OI8BX6","colab_type":"code","colab":{}},"source":["#calulating accuracy for training split and validation split\n","training_acc = calc_acc(X_train, y_train)\n","validation_acc = calc_acc(X_test, y_test)\n","print(training_acc)\n","print(validation_acc)"],"execution_count":0,"outputs":[]}]}